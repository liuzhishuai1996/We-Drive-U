{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from scipy.optimize import minimize\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMDP_train():\n",
    "    def __init__(self, action_space, delta, xi_norm, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        self.state_space = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "        self.action_space = action_space\n",
    "        self.initial_state = 'x1'\n",
    "        self.theta = [np.zeros(4), \n",
    "                      np.array([0,0,0,1]), \n",
    "                      np.array([0,0,0,1])]\n",
    "        self.delta = delta\n",
    "        Xi = np.full(len(self.action_space[0]), 1)\n",
    "        self.Xi = xi_norm * Xi / np.linalg.norm(Xi, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.S = [self.initial_state]\n",
    "        self.A = [] # save the action history\n",
    "        self.R = [] # save the reward history\n",
    "        self.h = 0 # reset the step to 0\n",
    "        self.feature = [] # save the feature trajectory\n",
    "        self.feature_a = [] # save the full feature trajectory\n",
    "        self.current_state = self.initial_state  # reset the current state to initial state\n",
    "    \n",
    "    def phi(self, current_state, A):\n",
    "        if current_state == 'x1':\n",
    "            phi = np.array([1 - self.delta - self.Xi @ A, 0, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x2':\n",
    "            phi = np.array([0, 1 - self.delta - self.Xi @ A, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x3':\n",
    "            phi = np.array([0, 0, 1 - self.delta - self.Xi @ A, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x4':\n",
    "            phi = np.array([0,0,1,0])\n",
    "        else:\n",
    "            phi = np.array([0,0,0,1])\n",
    "        return phi\n",
    "\n",
    "    def add_state(self, s): \n",
    "        self.S.append(s)\n",
    "        self.current_state = s\n",
    "\n",
    "    def update_state(self, phi, h):\n",
    "        # claculate the transition probability\n",
    "        if h == 0:\n",
    "            prob = [phi[0] * (1-0.001), 0, phi[0]* 0.001, phi[3]]\n",
    "        elif h == 1:\n",
    "            if phi[2] == 1:\n",
    "                prob = [0, 0, 1, 0]\n",
    "            elif phi[3] == 1:\n",
    "                prob = [0, 0, 0, 1]\n",
    "            else:\n",
    "                prob = [0, phi[1] * (1 - 0.001), phi[1]* 0.001, phi[3]]\n",
    "        else:\n",
    "            prob = [0, 0, phi[2], phi[3]]\n",
    "        sprime = np.random.choice(range(1,5), size=1, p=prob)[0]\n",
    "        return self.state_space[sprime] # return a string\n",
    "\n",
    "    def next_state(self, phi):\n",
    "        next_state = self.update_state(phi, self.h)\n",
    "        self.add_state(next_state)\n",
    "        return next_state\n",
    "\n",
    "    def generate_reward(self, phi):\n",
    "        reward = np.dot(phi, self.theta[self.h])\n",
    "        self.R.append(reward)\n",
    "        return reward\n",
    "\n",
    "    def step(self, a):\n",
    "        self.A.append(a)\n",
    "        phi = self.phi(self.current_state, a)\n",
    "        phi_a = [self.phi(self.current_state, a) for a in self.action_space]\n",
    "        self.feature.append(phi)\n",
    "        self.feature_a.append(phi_a)\n",
    "        self.generate_reward(phi)\n",
    "        self.next_state(phi)\n",
    "        self.h += 1\n",
    "    \n",
    "class LinearMDP_test():\n",
    "    \"\"\"Pertubed environment\"\"\"\n",
    "    def __init__(self, nominal_MDP, q, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        self.state_space = nominal_MDP.state_space\n",
    "        self.action_space = nominal_MDP.action_space\n",
    "        self.initial_state = 'x1'\n",
    "        self.theta = nominal_MDP.theta\n",
    "        self.delta = nominal_MDP.delta\n",
    "        self.Xi = nominal_MDP.Xi\n",
    "        self.q = q\n",
    "\n",
    "    def reset(self):\n",
    "        self.S = [self.initial_state] # save the feature trajectory\n",
    "        self.A = [] # save the action history\n",
    "        self.R = [] # save the reward history\n",
    "        self.h = 0 # reset the step to 0\n",
    "        self.feature = [] # save the feature trajectory\n",
    "        self.feature_a = [] # save the full feature trajectory\n",
    "        self.current_state = self.initial_state  # reset the current state to initial state\n",
    "\n",
    "    def phi(self, current_state, A):\n",
    "        if current_state == 'x1':\n",
    "            phi = np.array([1 - self.delta - self.Xi @ A, 0, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x2':\n",
    "            phi = np.array([0, 1 - self.delta - self.Xi @ A, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x3':\n",
    "            phi = np.array([0, 0, 1- self.delta - self.Xi @ A, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x4':\n",
    "            phi = np.array([0,0,1,0])\n",
    "        else:\n",
    "            phi = np.array([0,0,0,1])\n",
    "        return phi\n",
    "\n",
    "    def add_state(self, s): \n",
    "        self.S.append(s)\n",
    "        self.current_state = s\n",
    "\n",
    "    def update_state(self, phi, h):\n",
    "        # claculate the transition probability --- perturbed\n",
    "        if h == 0:\n",
    "            prob = [phi[0], 0, self.q * phi[3], (1 - self.q) * phi[3]]\n",
    "        elif h == 1:\n",
    "            if phi[2] == 1:\n",
    "                prob = [0, 0, 1, 0]\n",
    "            elif phi[3] == 1:\n",
    "                prob = [0, 0, 0, 1]\n",
    "            else:\n",
    "                prob = [0, phi[1] * (1 - 0.001), phi[1]* 0.001, phi[3]]\n",
    "        else:\n",
    "            prob = [0, 0, phi[2], phi[3]]\n",
    "        sprime = np.random.choice(range(1,5), size=1, p=prob)[0]\n",
    "        return self.state_space[sprime] # return a string\n",
    "\n",
    "    def next_state(self, phi):\n",
    "        next_state = self.update_state(phi, self.h)\n",
    "        self.add_state(next_state)\n",
    "        return next_state\n",
    "\n",
    "    def generate_reward(self, phi):\n",
    "        reward = np.dot(phi, self.theta[self.h])\n",
    "        self.R.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.A.append(a)\n",
    "        phi = self.phi(self.current_state, a)\n",
    "        phi_a = [self.phi(self.current_state, a) for a in self.action_space]\n",
    "        self.feature.append(phi)\n",
    "        self.feature_a.append(phi_a)\n",
    "        self.generate_reward(phi)\n",
    "        self.next_state(phi)\n",
    "        self.h += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSVI_UCB():\n",
    "    def __init__(self, A, beta, H, lam, fail_state=None):\n",
    "        self.lam = lam\n",
    "        self.H = H\n",
    "        self.action_space = A\n",
    "        self.beta = beta\n",
    "        self.w = [np.zeros(4) for _ in range(self.H)]\n",
    "        self.Lambda = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.fail_state = fail_state\n",
    "\n",
    "    def get_action(self, phi_a,  h):\n",
    "        Q_h = [self.get_Q_func(phi_a[idx], h) for idx in range(len(self.action_space))]\n",
    "        return self.action_space[np.argmax(Q_h)]\n",
    "    \n",
    "    def get_Q_func(self, phi, h, s_f=False):\n",
    "        if s_f == True:\n",
    "            return 0\n",
    "        else:\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            Q_h = np.min([(self.w[h] @ phi + self.beta * np.sqrt(phi @ Lambda_h_inverse @ phi)), self.H])\n",
    "            return Q_h \n",
    "\n",
    "    def update_Q(self, history): \n",
    "        # Backward induction\n",
    "        self.w = [None for _ in range(self.H)] # initialize weights w\n",
    "        for h in range(self.H-1, -1, -1):\n",
    "            # update Lambda_h\n",
    "            phi_h = history['phi'][-1][h]\n",
    "            self.Lambda[h] += np.outer(phi_h, phi_h)\n",
    "            # update w_h \n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            w_h = np.zeros(4)\n",
    "            if h == self.H - 1:\n",
    "                for tau in range(history['k']):\n",
    "                    phi_tau_h = history['phi'][tau][h]\n",
    "                    r_tau_h = history['r'][tau][h]\n",
    "                    w_h += Lambda_h_inverse @ (phi_tau_h * r_tau_h)\n",
    "            else:\n",
    "                for tau in range(history['k']):\n",
    "                    phi_tau_h = history['phi'][tau][h]\n",
    "                    phi_tau_h_plus_one = history['phi_a'][tau][h+1]\n",
    "                    r_tau_h = history['r'][tau][h]\n",
    "                    s_f_h_plus_one = (history['state'][tau][h+1] == self.fail_state)\n",
    "                    Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h + 1, s_f_h_plus_one)\n",
    "                                        for idx in range(len(self.action_space))]\n",
    "                    V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "                    w_h += Lambda_h_inverse @ (phi_tau_h * (r_tau_h + V_tau_h_plus_one))   \n",
    "            self.w[h] = w_h         \n",
    "\n",
    "class DR_LSVI_UCB():\n",
    "    def __init__(self, A, beta, H, lam, Rho, theta, fail_state=None):\n",
    "        self.lam = lam\n",
    "        self.H = H\n",
    "        self.action_space = A\n",
    "        self.beta = beta\n",
    "        self.w = [np.zeros(4) for _ in range(self.H)]\n",
    "        self.Lambda = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.Rho = Rho\n",
    "        self.theta = theta\n",
    "        self.fail_state = fail_state\n",
    "\n",
    "    def get_action(self, phi_a,  h):\n",
    "        Q_h = [self.get_Q_func(phi_a[idx], h) for idx in range(len(self.action_space))]\n",
    "        return self.action_space[np.argmax(Q_h)]\n",
    "    \n",
    "    def get_Q_func(self, phi, h, s_f=False):\n",
    "        if s_f == True:\n",
    "            return 0\n",
    "        else:\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            bonus =  self.beta * np.sqrt(phi @ np.diag(np.diagonal(Lambda_h_inverse)) @ phi)\n",
    "            Q_h = np.min([(self.w[h] @ phi + bonus), self.H - h])\n",
    "            return Q_h \n",
    "        \n",
    "    def get_nu_h(self, history, h, rho):\n",
    "        Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "        nu_h = np.zeros(4)  \n",
    "        Phi_h = np.zeros((0,4)) \n",
    "        V_h_plus_one = np.zeros(0)\n",
    "        for tau in range(history['k']): \n",
    "            phi_tau_h = history['phi'][tau][h]\n",
    "            Phi_h = np.vstack((Phi_h, phi_tau_h))\n",
    "            phi_tau_h_plus_one = history['phi_a'][tau][h+1]\n",
    "            s_f_h_plus_one = (history['state'][tau][h+1] == self.fail_state)\n",
    "            Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h + 1, s_f_h_plus_one) \n",
    "                                for idx in range(len(self.action_space))]\n",
    "            V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "            V_h_plus_one = np.hstack((V_h_plus_one, V_tau_h_plus_one))\n",
    "        for i in range(4):\n",
    "            def z_alpha_i(alpha):\n",
    "                # compact formular for z\n",
    "                z = Lambda_h_inverse @ Phi_h.T @ np.minimum(V_h_plus_one, alpha)\n",
    "                return -z[i] + rho[i]*alpha\n",
    "            result =  minimize(z_alpha_i, self.H/2, method='Nelder-Mead', bounds=[(0,self.H)])\n",
    "            nu_h[i] = - result.fun\n",
    "            #print(result.x)\n",
    "        return nu_h\n",
    "    \n",
    "    def update_Q(self, history): \n",
    "        # Backward induction\n",
    "        self.w = [None for _ in range(self.H)] # initialize weights w\n",
    "        for h in range(self.H-1, -1, -1):\n",
    "            # update Lambda_h\n",
    "            phi_h = history['phi'][-1][h]\n",
    "            self.Lambda[h] += np.outer(phi_h, phi_h)\n",
    "            # update w_h \n",
    "            w_h = np.zeros(4)\n",
    "            nu_h = np.zeros(4)\n",
    "            if h == self.H - 1:\n",
    "                w_h = self.theta[h]\n",
    "            else:\n",
    "                nu_h = self.get_nu_h(history, h, rho=self.Rho[h])\n",
    "                w_h = self.theta[h] + nu_h\n",
    "            self.w[h] = w_h  \n",
    "\n",
    "class VA_DR_LSVI_UCB():\n",
    "    def __init__(self, A, beta, H, lam, Rho, theta, fail_state=None):\n",
    "        self.lam = lam\n",
    "        self.H = H\n",
    "        self.action_space = A\n",
    "        self.beta = beta\n",
    "        self.w = [np.zeros(4) for _ in range(self.H)]\n",
    "        self.Lambda = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.Lambda_VA = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.Lambda_VA_last = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.Rho = Rho\n",
    "        self.theta = theta\n",
    "        self.fail_state = fail_state\n",
    "        self.variance = {}\n",
    "        self.switch = 0\n",
    "\n",
    "    def get_action(self, phi_a,  h):\n",
    "        Q_h = [self.get_Q_func(phi_a[idx], h) for idx in range(len(self.action_space))]\n",
    "        return self.action_space[np.argmax(Q_h)]\n",
    "    \n",
    "\n",
    "    def get_variance_coefficient(self, h, history):\n",
    "        Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "        z1_h = np.zeros(4)\n",
    "        z2_h = np.zeros(4)\n",
    "        Phi_h = np.zeros((0,4))\n",
    "        V_h_plus_one = np.zeros(0)\n",
    "        for tau in range(history['k']):\n",
    "            phi_tau_h = history['phi'][tau][h]\n",
    "            Phi_h = np.vstack((Phi_h, phi_tau_h))\n",
    "            phi_tau_h_plus_one = history['phi_a'][tau][h+1]\n",
    "            s_f_h_plus_one = (history['state'][tau][h+1] == self.fail_state)\n",
    "            Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h+1, s_f_h_plus_one)\n",
    "                                for idx in range(len(self.action_space))]\n",
    "            V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "            V_h_plus_one = np.hstack((V_h_plus_one, V_tau_h_plus_one))\n",
    "        \n",
    "        z1_h = Lambda_h_inverse @ Phi_h.T @ V_h_plus_one\n",
    "        z2_h = Lambda_h_inverse @ Phi_h.T @ V_h_plus_one**2\n",
    "        return z1_h, z2_h\n",
    "    \n",
    "    def estimated_variance(self, phi, z1_h, z2_h):\n",
    "        second_order_term = np.min([np.max([0, np.dot(phi, z2_h)]), self.H**2])\n",
    "        first_order_term = np.min([np.max([0, np.dot(phi, z1_h)]), self.H])\n",
    "        sigma_square = np.max([1, second_order_term - first_order_term**2])\n",
    "        return sigma_square\n",
    "\n",
    "    def get_Q_func(self, phi, h, s_f=False):\n",
    "        if s_f == True:\n",
    "            return 0\n",
    "        else:\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda_VA[h])\n",
    "            bonus =  self.beta * np.sqrt(phi @ np.diag(np.diagonal(Lambda_h_inverse)) @ phi)\n",
    "            Q_h = np.min([(self.w[h] @ phi + bonus), self.H - h])\n",
    "            return Q_h \n",
    "\n",
    "    def get_nu_h(self, history, h, rho, variance):\n",
    "        Lambda_h_inverse = np.linalg.inv(self.Lambda_VA[h])\n",
    "        nu_h = np.zeros(4)\n",
    "        Phi_h = np.zeros((0, 4))\n",
    "        V_h_plus_one = np.zeros(0)\n",
    "        for tau in range(history['k']):\n",
    "            phi_tau_h = history['phi'][tau][h]\n",
    "            Phi_h = np.vstack((Phi_h, phi_tau_h))\n",
    "            phi_tau_h_plus_one = history['phi_a'][tau][h+1]\n",
    "            s_f_h_plus_one = (history['state'][tau][h+1] == self.fail_state)\n",
    "            Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h+1, s_f_h_plus_one)\n",
    "                                for idx in range(len(self.action_space))]\n",
    "            V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "            V_h_plus_one = np.hstack((V_h_plus_one, V_tau_h_plus_one))\n",
    "        for i in range(4):\n",
    "            def z_alpha_i(alpha):\n",
    "                # compact formular for z\n",
    "                z = Lambda_h_inverse @ Phi_h.T @ (np.minimum(V_h_plus_one, alpha) / variance)\n",
    "                return -z[i] + rho[i] * alpha\n",
    "            result = minimize(z_alpha_i, self.H/2, method='Nelder-Mead', bounds=[(0, self.H)])\n",
    "            nu_h[i] = - result.fun\n",
    "        return nu_h\n",
    "\n",
    "    def update_covariance_matrix(self, history):\n",
    "        # Update Lambda_h and Lambda_VA_h\n",
    "        for h in range(self.H-1, -1, -1):\n",
    "            # update Lambda_h\n",
    "            phi_h = history['phi'][-1][h]\n",
    "            self.Lambda[h] += np.outer(phi_h, phi_h)\n",
    "            # update Lambda_VA_h\n",
    "            if h == self.H-1:\n",
    "                self.Lambda_VA[h] += np.outer(phi_h, phi_h)\n",
    "            else:\n",
    "                # calculate variance estimator\n",
    "                z1_h, z2_h = self.get_variance_coefficient(h, history)\n",
    "                self.variance[str(h)] = np.zeros(history['k'])\n",
    "                for tau in range(history['k']):\n",
    "                    feature_temp = history['phi'][tau][h]\n",
    "                    variance_temp = self.estimated_variance(feature_temp, z1_h, z2_h)\n",
    "                    self.variance[str(h)][tau] = variance_temp\n",
    "                    self.Lambda_VA[h] += np.outer(feature_temp, feature_temp) / variance_temp\n",
    "\n",
    "    def update_Q(self, history):\n",
    "        # check if the criterion is met\n",
    "        criterion = False\n",
    "        for h in range(self.H):\n",
    "            if np.linalg.det(self.Lambda_VA[h]) >= 2 * np.linalg.det(self.Lambda_VA_last[h]):\n",
    "                criterion = True\n",
    "                break\n",
    "\n",
    "        # if criterion is met\n",
    "        if criterion:            \n",
    "            # number of switch + 1\n",
    "            self.switch += 1\n",
    "\n",
    "            # Backward induction\n",
    "            self.w = [None for _ in range(self.H)]\n",
    "            for h in range(self.H-1, -1, -1):\n",
    "                # update Lambda_VA_last\n",
    "                self.Lambda_VA_last[h] = copy.copy(self.Lambda_VA[h])\n",
    "                \n",
    "                # update w_h\n",
    "                w_h = np.zeros(4)\n",
    "                nu_h = np.zeros(4)\n",
    "                if h == self.H - 1:\n",
    "                    w_h = self.theta[h]\n",
    "                else:\n",
    "                    variance = self.variance[str(h)]\n",
    "                    nu_h = self.get_nu_h(history, h, rho = self.Rho[h], variance=variance)\n",
    "                    w_h = self.theta[h] + nu_h\n",
    "                self.w[h] = w_h\n",
    "        # if criterion is not met\n",
    "        else:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(epoch, action_space, beta, delta, xi_norm, H, lam, fail_state, seed):\n",
    "    history = {'k': 0, 'phi':[], 'r':[], 'state':[], 'phi_a':[]}\n",
    "    env = LinearMDP_train(action_space, delta, xi_norm, seed=seed)\n",
    "    agent = LSVI_UCB(A=action_space, beta=beta, H=H, lam=lam, fail_state=fail_state)\n",
    "    reward = 0\n",
    "    Reward = []\n",
    "    for t in range(epoch):\n",
    "        env.reset()\n",
    "        for h in range(H):\n",
    "            current_state = env.current_state\n",
    "            phi_a = [env.phi(current_state, a) for a in action_space]\n",
    "            action = agent.get_action(phi_a, h)\n",
    "            env.step(action)\n",
    "        # log the trajectory\n",
    "        history['phi_a'].append(env.feature_a)\n",
    "        history['phi'].append(env.feature)\n",
    "        history['r'].append(env.R)\n",
    "        history['state'].append(env.S)\n",
    "        history['k'] += 1\n",
    "        # update the agent\n",
    "        agent.update_Q(history)\n",
    "        reward += np.sum(env.R)\n",
    "        Reward.append(reward)\n",
    "    return agent\n",
    "\n",
    "\n",
    "def train_once_DR(epoch, action_space, beta, delta, xi_norm, Rho, H, lam, fail_state, seed):\n",
    "    history = {'k': 0, 'phi':[], 'r':[], 'state':[], 'phi_a':[]}\n",
    "    env = LinearMDP_train(action_space, delta, xi_norm, seed=seed)\n",
    "    DR_agent = DR_LSVI_UCB(A=action_space, beta=beta, H=H, lam=lam, Rho=Rho, theta=env.theta, fail_state=fail_state)\n",
    "    reward = 0\n",
    "    Reward = []\n",
    "    for t in range(epoch):\n",
    "        env.reset()\n",
    "        for h in range(H):\n",
    "            current_state = env.current_state\n",
    "            phi_a = [env.phi(current_state, a) for a in action_space]\n",
    "            action = DR_agent.get_action(phi_a, h)\n",
    "            env.step(action)\n",
    "        # log the trajectory\n",
    "        history['phi_a'].append(env.feature_a)\n",
    "        history['phi'].append(env.feature)\n",
    "        history['r'].append(env.R)\n",
    "        history['state'].append(env.S)\n",
    "        history['k'] += 1\n",
    "        # update the agent\n",
    "        DR_agent.update_Q(history)\n",
    "        reward += np.sum(env.R)\n",
    "        Reward.append(reward)\n",
    "    return DR_agent\n",
    "\n",
    "\n",
    "def train_once_DR_VA(epoch, action_space, beta, delta, xi_norm, Rho, H, lam, fail_state, seed):\n",
    "    history = {'k': 0, 'phi':[], 'r':[], 'state':[], 'phi_a':[]}\n",
    "    env = LinearMDP_train(action_space, delta, xi_norm, seed=seed)\n",
    "    VA_DR_agent = VA_DR_LSVI_UCB(A=action_space, beta=beta, H=H, lam=lam, Rho=Rho, theta=env.theta, fail_state=fail_state)\n",
    "    reward = 0\n",
    "    Reward = []\n",
    "    for t in range(epoch):\n",
    "        env.reset()\n",
    "        for h in range(H):\n",
    "            current_state = env.current_state\n",
    "            phi_a = [env.phi(current_state, a) for a in action_space]\n",
    "            action = VA_DR_agent.get_action(phi_a, h)\n",
    "            env.step(action)\n",
    "        # log the trajectory\n",
    "        history['phi_a'].append(env.feature_a)\n",
    "        history['phi'].append(env.feature)\n",
    "        history['r'].append(env.R)\n",
    "        history['state'].append(env.S)\n",
    "        history['k'] += 1\n",
    "        # update the agent\n",
    "        VA_DR_agent.update_covariance_matrix(history)\n",
    "        VA_DR_agent.update_Q(history)\n",
    "        reward += np.sum(env.R)\n",
    "        Reward.append(reward)\n",
    "    return VA_DR_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 200\n",
    "H = 3\n",
    "rho = 0.3\n",
    "beta = 1\n",
    "lam = 0.1\n",
    "actions = list(product([-1, 1], repeat=4))\n",
    "action_space = [np.array(action) for action in actions]\n",
    "delta = 0.3\n",
    "xi_norm = 0.1\n",
    "Rho = [[0,0,0,rho], [0,0,0,0]]\n",
    "fail_state = 'x4'\n",
    "replication = 10\n",
    "agent_dic = {}\n",
    "DR_agent_dic = {}\n",
    "VA_DR_agent_dic = {}\n",
    "\n",
    "begin_time_LSVI = time.time()\n",
    "for rep in range(replication):\n",
    "    agent = train_once(epoch=T1, action_space=action_space, beta=beta, delta=delta, \n",
    "                       xi_norm=xi_norm, H=H, lam=lam, fail_state=fail_state, seed=rep)\n",
    "    agent_dic[str(rep)] = agent\n",
    "end_time_LSVI = time.time()\n",
    "\n",
    "begin_time_DR_LSVI = time.time()\n",
    "for rep in range(replication):\n",
    "    DR_agent = train_once_DR(epoch=T1, action_space=action_space, beta=beta, delta=delta, \n",
    "                             xi_norm=xi_norm, Rho=Rho, H=H, lam=lam, fail_state=fail_state, seed=rep)\n",
    "    DR_agent_dic[str(rep)] = DR_agent\n",
    "end_time_DR_LSVI = time.time()\n",
    "\n",
    "begin_time_VA_DR_LSVI = time.time()\n",
    "for rep in range(replication):\n",
    "    VA_DR_agent = train_once_DR_VA(epoch=T1, action_space=action_space, beta=beta, delta=delta, \n",
    "                                   xi_norm=xi_norm, Rho=Rho, H=H, lam=lam, fail_state=fail_state, seed=rep)\n",
    "    VA_DR_agent_dic[str(rep)] = VA_DR_agent\n",
    "end_time_VA_DR_LSVI = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.2\n"
     ]
    }
   ],
   "source": [
    "switch_times = []\n",
    "for rep in range(replication):\n",
    "    switch_times.append(VA_DR_agent_dic[str(rep)].switch)\n",
    "\n",
    "print(np.average(switch_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perturbation = [x / 20 for x in range(21)]\n",
    "T2 = 100\n",
    "R_LSVI_UCB = []\n",
    "R_DR_LSVI_UCB = []\n",
    "R_VA_DR_LSVI_UCB = []\n",
    "env = LinearMDP_train(action_space, delta, xi_norm)\n",
    "for q in Perturbation:\n",
    "    REWARD = 0\n",
    "    REWARD_DR = 0\n",
    "    REWARD_DR_VA = 0\n",
    "    for rep in range(replication):\n",
    "        reward = 0\n",
    "        reward_DR = 0 \n",
    "        reward_DR_VA = 0\n",
    "        env_test = LinearMDP_test(env, q=q, seed=rep)\n",
    "        env_test_DR = LinearMDP_test(env, q=q, seed=rep)\n",
    "        env_test_DR_VA = LinearMDP_test(env, q=q, seed=rep)\n",
    "        agent = agent_dic[str(rep)]\n",
    "        DR_agent = DR_agent_dic[str(rep)]\n",
    "        VA_DR_agent = VA_DR_agent_dic[str(rep)]\n",
    "        \n",
    "        \n",
    "        for t in range(T2):\n",
    "            env_test.reset()\n",
    "            env_test_DR.reset()\n",
    "            env_test_DR_VA.reset()\n",
    "            for h in range(H):\n",
    "                # LSVI-UCB\n",
    "                current_state = env_test.current_state\n",
    "                phi = [env_test.phi(current_state, a) for a in action_space]\n",
    "                action = agent.get_action(phi, h)\n",
    "                env_test.step(action)\n",
    "\n",
    "                # DR-LSVI-UCB\n",
    "                current_state_DR = env_test_DR.current_state\n",
    "                phi_DR = [env_test_DR.phi(current_state_DR, a) for a in action_space]\n",
    "                action_DR = DR_agent.get_action(phi_DR, h)\n",
    "                env_test_DR.step(action_DR)\n",
    "                \n",
    "                # VA-DR-LSVI-UCB\n",
    "                current_state_DR_VA = env_test_DR_VA.current_state\n",
    "                phi_DR_VA = [env_test_DR_VA.phi(current_state_DR_VA, a) for a in action_space]\n",
    "                action_DR_VA = VA_DR_agent.get_action(phi_DR_VA, h)\n",
    "                env_test_DR_VA.step(action_DR_VA)\n",
    "\n",
    "            reward += np.sum(env_test.R)/T2\n",
    "            reward_DR += np.sum(env_test_DR.R)/T2\n",
    "            reward_DR_VA += np.sum(env_test_DR_VA.R)/T2\n",
    "\n",
    "        REWARD += reward / replication\n",
    "        REWARD_DR += reward_DR / replication\n",
    "        REWARD_DR_VA += reward_DR_VA / replication\n",
    "\n",
    "    R_LSVI_UCB.append(REWARD)\n",
    "    R_DR_LSVI_UCB.append(REWARD_DR)\n",
    "    R_VA_DR_LSVI_UCB.append(REWARD_DR_VA)\n",
    "plt.plot(Perturbation, R_LSVI_UCB, label = 'LSVI-UCB')\n",
    "plt.plot(Perturbation, R_DR_LSVI_UCB, label = 'DR-LSVI-UCB')\n",
    "plt.plot(Perturbation, R_VA_DR_LSVI_UCB, label = 'We-DRIVE-U')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('Perturbation', size=16)\n",
    "plt.ylabel('Average reward', size=16)\n",
    "# plt.savefig(f'robustness_{delta}_{xi_norm}_{rho}.pdf', dpi=1000, bbox_inches='tight', pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
